{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset wrangled in this project is the twitter archive of the user We rate dogs,which rates peoples dogs by providing humorous comment about the dog.The project goals included:\n",
    "        1.Wrangling the data which involved:\n",
    "           - Gathering data\n",
    "           - Assessing data\n",
    "           - Cleaning data \n",
    "        2.Storing and analyzing your wrangled data\n",
    "        3.Reporting on your wrangling steps and insights from analyzed data.\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data\n",
    "* The data was gathered from 3 different sources:\n",
    "    - WeRate dogs twitter archive which was provided  as a csv file.I read and       uploaded it to jupyter notebook,and read it into  a pandas dataframe           known as twitterarchive\n",
    "    - The tweet image predictions was downloaded programmatically using the           requests libray.A url was provided to download the content.The tweet           image preditions file was tab separated which I stored in a csv                 file called imagestable.\n",
    "      \n",
    "    - Additional resource from the twitter API. I used the tweet_json.txt file       provided in the classroom, read the contents of the txt file to a list         and stored the list in a pandas dataframe,tweet_info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Assessing data\n",
    "*  I used both visual and programmatic assessmnet to assess the data,the following programmatic assessments were used:\n",
    "     - *.head()*\n",
    "     - *.info()*\n",
    "     - *.sample()*\n",
    "     - *.tail()*\n",
    "     - *.value_counts()*\n",
    "     - *.duplicated()*\n",
    "     \n",
    "* The following issues were noted :\n",
    "    1. Quality issues\n",
    "      - Missing data-In reply to status id ,in reply to user idcolumn,retweeted         status id,retweeted status user id have some null values,for the               twitter archive data.\n",
    "      - Duplicate data- The source column contained 2352 rows of duplicated             data,twitter archive \n",
    "      - Incorrect data-Some of the dog names were incorrect, including names           such as 'a','the','very' ,twitter archive sample. The p1,p2,p3                 predictions also contained some images that were not dogs ,ie electric         fans,images prediction file.\n",
    "      - Inconsistent data- Some of the names in p1,p2 and p3 columns started           with lower case while others started with upper case,some of the rating \n",
    "         denominators are not equal to 10.\n",
    "      - Wrong data types - tweet id and timestamp were integers and strings,           instead of strings and datetime datatypes\n",
    "    2. Structural issues.\n",
    "       - Column headers are values, not variable names:doggo,floofer,pupper,             poppo are dog stages and can be placed under one  column ,instead of           4 separate columns.\n",
    "       - The images prediction table,the twitter archive table, the retweet and \n",
    "          favorite count should be merged together to form one data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "To resolve quality issues,the following steps were taken:\n",
    "       1. *Define where I wrote what I was going to do to clean the data\n",
    "       2. \"Code turning the define step to code.\n",
    "       3. *Testing :just to make sure my cleaning code worked.\n",
    "\n",
    "**Define\n",
    "\n",
    "  i) Created a copy of the 3 datasets.\n",
    " ii) Created a list of wrong names and used replace() to substitute wrong            dog names with None \n",
    " iii) Created a list where the denominator is not equal to 10 &drop rows with         denominators not equal to 10\n",
    " iv) Quered for retweeted status id, drop those with non-null values. Drop          retweeted status timestamp, retweeted status user id, retweeted status id      since those were redundant columns.\n",
    "  v) Get in_reply to user id and in reply to status id columns, use drop to           drop them\n",
    "  vi) Change timestamp to datetime using astype,change tweet id to string type\n",
    "  vii)  Drop source column using .drop()\n",
    "  viii) Replace first uppercase letters in p1,p2,p3 with lowercase using                title()\n",
    "  ix) Find rows that have all false and drop them, define a function to check         true or false with p1_dog,p2_dog\n",
    "  x) Combine doggo,floofer,pupper and puppo into dog stage column, then drop         the 4 columns\n",
    "  xi) Merge retweet count, favorite count on the twitter archive table using         tweet_id.\n",
    " \n",
    "**Code and Test\n",
    "\n",
    "Most of the code and tests  is shown on the wrangle_act.ipynb.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and analyzing your data\n",
    "\n",
    "The clean data is stored in on a csv file twitter archive master.csv\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
